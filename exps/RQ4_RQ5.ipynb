{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17b0bacb-153d-4945-b7c3-fb997f599f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a61cd705-b76a-4de7-ad8c-afb6513bbefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded.....\n",
      "==========>\n",
      "dataset saved.....\n",
      "==========>\n",
      "[' anoth blueprint sampl contribut day test blueprint mbean gener complex blueprint sampl sampl user learn defin nest compon blueprint xml run sampl besid osgi framework bundl requir coreopt mavenbundl groupid org apach felix artifactid org apach felix configadmin versionasinproject coreopt mavenbundl groupid org apach felix artifactid org apach felix eventadmin versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log api versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log servic versionasinproject coreopt mavenbundl groupid org apach ari blueprint artifactid org apach ari blueprint versionasinproject coreopt mavenbundl groupid org apach ari artifactid org apach ari util coreopt mavenbundl groupid org apach ari jmx artifactid ari jmx blueprint versionasinproject', 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    f = open('./src/dataset/final_dataset.pickle', 'rb')\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "    print(\"dataset loaded.....\")\n",
    "    print('==========>')\n",
    "\n",
    "\n",
    "    binary_class_dataset = []\n",
    "    for index in range(len(dataset)):\n",
    "        task = []\n",
    "        task.append(dataset[index][0])\n",
    "        if str(dataset[index][1][0]).strip() == 'none':\n",
    "            ref = 0\n",
    "        else:\n",
    "            ref = 1\n",
    "        task.append(ref)\n",
    "\n",
    "        binary_class_dataset.append(task)\n",
    "\n",
    "    f = open('./src/dataset/binary_class_dataset.pickle', 'wb')\n",
    "    pickle.dump(binary_class_dataset,f)\n",
    "    f.close()\n",
    "    print(\"dataset saved.....\")\n",
    "    print('==========>')\n",
    "\n",
    "    print(binary_class_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4ec8360-fbb6-4fc8-badb-a40085ee544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature words\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cdf5c5d-4226-4f0f-966d-83f36a823b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text.lower();\n",
    "    # Get the difference of all ASCII characters from the set of printable characters\n",
    "    nonprintable = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "    # Use translate to remove all non-printable characters\n",
    "    return text.translate({ord(character): None for character in nonprintable})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c36c7c2-3ebc-4733-bcf4-08f09b16f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    return word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7ad2f27-38a8-4a89-b875-e817345daa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "\n",
    "    return pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e07cc92-5850-425a-a6bc-39002de42aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(pos_tags):\n",
    "    adjective_tags = ['JJ', 'JJR', 'JJS']\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = []\n",
    "    for word in pos_tags:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0], pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))  # default POS = noun\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "496151a5-0414-4536-acd8-fd4501c631f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(pos_tags_lem, lem_text):\n",
    "    stopwords = []\n",
    "\n",
    "    wanted_POS = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'VBG', 'FW']\n",
    "\n",
    "    for word in pos_tags_lem:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "\n",
    "    punctuations = list(str(string.punctuation))\n",
    "\n",
    "    stopwords = stopwords + punctuations\n",
    "\n",
    "    stopword_file = open(\"./dataset/long_stopwords.txt\", \"r\")\n",
    "    # Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "    lots_of_stopwords = []\n",
    "\n",
    "    for line in stopword_file.readlines():\n",
    "        lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "    stopwords_plus = []\n",
    "    stopwords_plus = stopwords + lots_of_stopwords\n",
    "    stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "    processed_text = []\n",
    "    for word in lem_text:\n",
    "        if word not in stopwords_plus:\n",
    "            processed_text.append(word)\n",
    "\n",
    "    return processed_text, stopwords_plus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1106c807-2d8e-4e39-a410-82befcf44750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anoth blueprint sampl contribut day test blueprint mbean gener complex blueprint sampl sampl user learn defin nest compon blueprint xml run sampl besid osgi framework bundl requir coreopt mavenbundl groupid org apach felix artifactid org apach felix configadmin versionasinproject coreopt mavenbundl groupid org apach felix artifactid org apach felix eventadmin versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log api versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log servic versionasinproject coreopt mavenbundl groupid org apach ari blueprint artifactid org apach ari blueprint versionasinproject coreopt mavenbundl groupid org apach ari artifactid org apach ari util coreopt mavenbundl groupid org apach ari jmx artifactid ari jmx blueprint versionasinproject', 0]\n",
      "tasks loaded.....\n",
      "all_phrases_without_freq saved......\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    load = open(\"./src/dataset/binary_class_dataset.pickle\", \"rb\")\n",
    "    tasks = pickle.load(load)\n",
    "    load.close()\n",
    "    print(tasks[0])\n",
    "    print(\"tasks loaded.....\")\n",
    "\n",
    "    preprocessed_tasks = []\n",
    "\n",
    "    all_words = []\n",
    "    freq_words_without_freq = []\n",
    "\n",
    "    for index in range(len(tasks)):\n",
    "\n",
    "        # without preprocessing\n",
    "        old_task = tasks[index]\n",
    "        text = tasks[index][0].lower()\n",
    "        cleaned_text = clean_text(text)\n",
    "        words = re.findall(r'\\w+', cleaned_text)\n",
    "\n",
    "               # without preprocessing\n",
    "        for item in words:\n",
    "\n",
    "            all_words.append(item)\n",
    "\n",
    "\n",
    "    counts = Counter(all_words).most_common(5000)\n",
    "\n",
    "    for index in range(len(counts)):\n",
    "        freq_words_without_freq.append(counts[index][0])\n",
    "\n",
    "\n",
    "    file = open(\"./src/dataset/freq_words_without_preprocessing.pickle\", \"wb\")\n",
    "    pickle.dump(freq_words_without_freq, file)\n",
    "    file.close()\n",
    "    print(\"all_phrases_without_freq saved......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9082f6f-ba7d-445e-af97-e844f00a9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature sets generation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4746df4-cf5f-488b-9a42-ce056746303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    file = open(\"./src/dataset/shuffle_dataset.pickle\", \"rb\")\n",
    "    dataset = pickle.load(file)\n",
    "    file.close()\n",
    "    print(\"dataset of length \" , len(dataset) , \" is loaded.....\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e23c3e26-dde7-453a-84df-06d74b9aa40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features():\n",
    "    # file = open(\"./src/dataset/features.pickle\", \"rb\")\n",
    "    file = open(\"./src/dataset/features_without_pp.pickle\", \"rb\")\n",
    "    features = pickle.load(file)\n",
    "    file.close()\n",
    "    print(\"features of length \" , len(features) , \" is loaded.....\")\n",
    "    # print(features[:100])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee4090e8-5d10-4985-9b18-c1a384bedfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(w_features, t_words):\n",
    "    words = t_words\n",
    "    features = {}\n",
    "    for f in w_features:\n",
    "        features[f] = 0\n",
    "\n",
    "    for f in features:\n",
    "        if f in words:\n",
    "            features[f] += 1\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b39eb373-f473-4af1-b645-0f486ef2b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_featuresets(word_features):\n",
    "\n",
    "    documents_f = open(\"./src/T&T/testing.pickle\", \"rb\")\n",
    "    testing_files = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    print(str(len(testing_files)) + \" testing files loaded.....\")\n",
    "\n",
    "    testing_featureset = []\n",
    "\n",
    "    for index in range(len(testing_files)):\n",
    "        testing_featureset.append(\n",
    "            [find_features(word_features, testing_files[index][2]),\n",
    "             testing_files[index][1]])\n",
    "\n",
    "    save_featuresets = open(\"./src/feature_modeling/testing_featureset.pickle\", \"wb\")\n",
    "    pickle.dump(testing_featureset, save_featuresets)\n",
    "    save_featuresets.close()\n",
    "    print(\"testing featuresets saved.....\")\n",
    "    print(\"===============>\")\n",
    "\n",
    "    for t_index in range(10):\n",
    "        documents_f = open(\"./src/T&T/training\" + str(t_index + 1) +\".pickle\", \"rb\")\n",
    "        training_files = pickle.load(documents_f)\n",
    "        documents_f.close()\n",
    "        print(str(len(training_files)) + \" training files loaded.....\")\n",
    "\n",
    "        training_featureset = []\n",
    "\n",
    "        for index in range(len(training_files)):\n",
    "            training_featureset.append(\n",
    "                [find_features(word_features, training_files[index][2]),\n",
    "                 training_files[index][1]])\n",
    "\n",
    "        save_featuresets = open(\"./src/feature_modeling/training_featureset\" + str(t_index + 1) + \".pickle\",\"wb\")\n",
    "        pickle.dump(training_featureset, save_featuresets)\n",
    "        save_featuresets.close()\n",
    "        print(\"training featuresets \" + str(t_index + 1) + \" saved.....\")\n",
    "        print(\"===============>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f9de1a0-439a-4aea-8b1c-0d685f107ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of length  5000  is loaded.....\n",
      "1336 testing files loaded.....\n",
      "testing featuresets saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 1 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 2 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 3 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 4 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 5 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 6 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 7 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 8 saved.....\n",
      "===============>\n",
      "1200 training files loaded.....\n",
      "training featuresets 9 saved.....\n",
      "===============>\n",
      "1228 training files loaded.....\n",
      "training featuresets 10 saved.....\n",
      "===============>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # dataset = load_dataset()\n",
    "    features = load_features()\n",
    "    # saperate_training_and_testing_data(dataset)\n",
    "    make_featuresets(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4333b489-80f4-430d-b457-8ccceda86e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactoring prediction\n",
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58a2bbe8-f749-491a-a19f-97de1d65edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of testing dataset:  1336\n"
     ]
    }
   ],
   "source": [
    "# classifiers = ['SVM', 'NB', 'MNB', 'BNB', 'LR', 'RF']\n",
    "classifiers = ['MNB']\n",
    "# data loading\n",
    "featuresets_f = open(\"./src/feature_modeling/testing_featureset.pickle\", \"rb\")\n",
    "testing_set = pickle.load(featuresets_f)\n",
    "featuresets_f.close()\n",
    "print(\"lenght of testing dataset: \", len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50f310e2-bff0-440b-a36c-aac7b1f38434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 1 done.....\n",
      "=========>>\n",
      "iteration 2\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 2 done.....\n",
      "=========>>\n",
      "iteration 3\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 3 done.....\n",
      "=========>>\n",
      "iteration 4\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 4 done.....\n",
      "=========>>\n",
      "iteration 5\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 5 done.....\n",
      "=========>>\n",
      "iteration 6\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 6 done.....\n",
      "=========>>\n",
      "iteration 7\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 7 done.....\n",
      "=========>>\n",
      "iteration 8\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 8 done.....\n",
      "=========>>\n",
      "iteration 9\n",
      "lenght of training dataset:  1200\n",
      "MNB for iteration 9 done.....\n",
      "=========>>\n",
      "iteration 10\n",
      "lenght of training dataset:  1228\n",
      "MNB for iteration 10 done.....\n",
      "=========>>\n"
     ]
    }
   ],
   "source": [
    "for t_index in range(10):\n",
    "\n",
    "    print(\"iteration \" + str(t_index + 1))\n",
    "    featuresets_f = open(\"./src/feature_modeling/training_featureset\" + str(t_index + 1) + \".pickle\", \"rb\")\n",
    "    training = pickle.load(featuresets_f)\n",
    "    featuresets_f.close()\n",
    "    print(\"lenght of training dataset: \", len(training))\n",
    "\n",
    "    for cls in classifiers:\n",
    "        if cls == 'SVM':\n",
    "            classifier = SklearnClassifier(LinearSVC())\n",
    "            classifier.train(training)\n",
    "        elif cls == 'NB':\n",
    "            classifier = nltk.NaiveBayesClassifier.train(training)\n",
    "            classifier.train(training)\n",
    "        elif cls == 'MNB':\n",
    "            classifier = SklearnClassifier(MultinomialNB())\n",
    "            classifier.train(training)\n",
    "        elif cls == 'BNB':\n",
    "            classifier = SklearnClassifier(BernoulliNB())\n",
    "            classifier.train(training)\n",
    "        elif cls == 'LR':\n",
    "            classifier = SklearnClassifier(LogisticRegression())\n",
    "            classifier.train(training)\n",
    "        elif cls == 'RF':\n",
    "            classifier = SklearnClassifier(RandomForestClassifier())\n",
    "            classifier.train(training)\n",
    "        # prediction\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for i, (feats, label_true) in enumerate(testing_set):\n",
    "            label_pred = classifier.classify(feats)\n",
    "            y_true.append(label_true)\n",
    "            y_pred.append(label_pred)\n",
    "\n",
    "        # save_classifier = open(\"./trained_classifiers/LRall.pickle\", \"wb\")\n",
    "        save_classifier = open(\"./src/trained_classifiers/\" + cls + str(t_index + 1) + \".pickle\", \"wb\")\n",
    "        pickle.dump(classifier, save_classifier)\n",
    "        save_classifier.close()\n",
    "\n",
    "        # save_classifier = open(\"./y_true_pred/y_true_LRall.pickle\", \"wb\")\n",
    "        save_classifier = open(\"./src/y_true_pred/y_true_\" + cls + str(t_index + 1) +  \".pickle\", \"wb\")\n",
    "        pickle.dump(y_true, save_classifier)\n",
    "        save_classifier.close()\n",
    "\n",
    "        # save_classifier = open(\"./y_true_pred/y_pred_LRall.pickle\", \"wb\")\n",
    "        save_classifier = open(\"./src/y_true_pred/y_pred_\" + cls + str(t_index + 1) +  \".pickle\", \"wb\")\n",
    "        pickle.dump(y_pred, save_classifier)\n",
    "        save_classifier.close()\n",
    "\n",
    "        print(cls + \" for iteration \" + str(t_index + 1) + \" done.....\")\n",
    "        print(\"=========>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b18c6fe-a410-4161-aa11-b3da0a24ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96a2fd62-2ba1-4e6c-8b67-814d3444b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi label classifiers\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyodbc\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bfd48ac-5f4b-4468-b441-e504c91bed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv('./src/dataset/TextPreprocessed.csv', encoding='iso-8859-1')\n",
    "# print(df_text.head())\n",
    "df_tags = pd.read_csv('./src/dataset/Tag.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c8681ee-1b13-47e4-9915-35620e044a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "grouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\n",
    "most_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\n",
    "df_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\n",
    "df_tags = df_tags.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4e05843-96ed-410f-9421-c054abdc7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/lddqgvp174vg4jn75krxfg9w0000gn/T/ipykernel_57613/160941740.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  firstlast = counts[:5].append(counts[-5:])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extract method                                ...</td>\n",
       "      <td>4225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rename method                                 ...</td>\n",
       "      <td>2940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>move method                                   ...</td>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>move attribute                                ...</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rename class                                  ...</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pull up attribute                             ...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extract interface                             ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>extract superclass                            ...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>push down method                              ...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>push down attribute                           ...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index  count\n",
       "0  extract method                                ...   4225\n",
       "1  rename method                                 ...   2940\n",
       "2  move method                                   ...   1441\n",
       "3  move attribute                                ...    957\n",
       "4  rename class                                  ...    801\n",
       "5  pull up attribute                             ...    186\n",
       "6  extract interface                             ...    149\n",
       "7  extract superclass                            ...    132\n",
       "8  push down method                              ...    112\n",
       "9  push down attribute                           ...    102"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df_tags.Tag.value_counts()\n",
    "firstlast = counts[:5].append(counts[-5:])\n",
    "firstlast.reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9f4c547-a942-462f-840a-7fa8648d2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_for_question(question_id):\n",
    "    return df_tags[df_tags['Id'] == question_id].Tag.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a910b4c0-3000-4d95-8d99-8c4f3a3709ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags_column(row):\n",
    "    row['Tags'] = tags_for_question(row['Id'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d14b3db-e9db-4e30-95dd-1150e11a23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_text.apply(add_tags_column, axis=1)\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_questions.Tags)\n",
    "Y = multilabel_binarizer.transform(df_questions.Tags)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(df_questions.Text.values.astype('U'))\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "ros = RandomOverSampler(random_state=9000)\n",
    "X_tfidf_resampled, Y_tfidf_resampled = ros.fit_resample(X_tfidf, Y)\n",
    "\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_resampled, Y_tfidf_resampled, test_size=0.2, random_state=9000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df34bb41-da6f-4df5-ac3d-c8ab2be9ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    print(y_true.shape[0])\n",
    "    print(y_pred)\n",
    "\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set(np.where(y_true[i])[0])\n",
    "        set_pred = set(np.where(y_pred[i])[0])\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            # tmp_a = len(set_true.union(set_pred))\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    # print(acc_list)\n",
    "    return np.mean(acc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5965f3b7-7279-411f-9e54-2b6fe378ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    # print(\"Hamming loss: {}\".format(hamming_loss(y_test_tfidf, y_pred)))\n",
    "    print(\"Hamming score: {}\".format(hamming_score(y_test_tfidf, y_pred)))\n",
    "    print('Subset accuracy: {0}'.format(accuracy_score(y_test_tfidf, y_pred, normalize=True, sample_weight=None)))\n",
    "    print('Subset precision: {0}'.format(precision_score(y_test_tfidf, y_pred, average='samples')))\n",
    "    print(\"---\")\n",
    "\n",
    "# sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
    "#lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "#svm = LinearSVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5b8272b-353b-4a8c-baa1-67ab8a3a136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  MultinomialNB\n",
      "7300\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Hamming score: 0.14112904979343338\n",
      "Subset accuracy: 0.14095890410958903\n",
      "Subset precision: 0.14112904979343338\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.31      0.46       535\n",
      "           1       0.27      0.01      0.01       525\n",
      "           2       0.93      0.34      0.50       531\n",
      "           3       0.64      0.03      0.06       539\n",
      "           4       0.85      0.17      0.29       511\n",
      "           5       0.73      0.05      0.10       519\n",
      "           6       0.76      0.07      0.12       544\n",
      "           7       0.64      0.04      0.07       499\n",
      "           8       0.91      0.21      0.35       494\n",
      "           9       0.83      0.09      0.16       512\n",
      "          10       0.95      0.38      0.54       519\n",
      "          11       0.94      0.25      0.39       507\n",
      "          12       0.69      0.05      0.10       547\n",
      "          13       0.36      0.01      0.02       518\n",
      "\n",
      "   micro avg       0.87      0.14      0.25      7300\n",
      "   macro avg       0.74      0.14      0.23      7300\n",
      "weighted avg       0.74      0.14      0.23      7300\n",
      " samples avg       0.14      0.14      0.14      7300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rakshitharozario/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
    "#lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "for classifier in [mn]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(x_train_tfidf, y_train_tfidf)\n",
    "    y_pred = clf.predict(x_test_tfidf)\n",
    "    print_score(y_pred, classifier)\n",
    "    print(classification_report(y_test_tfidf, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0151376-210d-44c5-a875-21574de4feb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
