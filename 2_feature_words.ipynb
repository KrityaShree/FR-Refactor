{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e31a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anoth blueprint sampl contribut day test blueprint mbean gener complex blueprint sampl sampl user learn defin nest compon blueprint xml run sampl besid osgi framework bundl requir coreopt mavenbundl groupid org apach felix artifactid org apach felix configadmin versionasinproject coreopt mavenbundl groupid org apach felix artifactid org apach felix eventadmin versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log api versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log servic versionasinproject coreopt mavenbundl groupid org apach ari blueprint artifactid org apach ari blueprint versionasinproject coreopt mavenbundl groupid org apach ari artifactid org apach ari util coreopt mavenbundl groupid org apach ari jmx artifactid ari jmx blueprint versionasinproject', 0]\n",
      "tasks loaded.....\n",
      "all_phrases_without_freq saved......\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text.lower();\n",
    "    # Get the difference of all ASCII characters from the set of printable characters\n",
    "    nonprintable = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "    # Use translate to remove all non-printable characters\n",
    "    return text.translate({ord(character): None for character in nonprintable})\n",
    "\n",
    "def tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def pos_tagging(text):\n",
    "\n",
    "    return pos_tag(text)\n",
    "\n",
    "def lemmatization(pos_tags):\n",
    "    adjective_tags = ['JJ', 'JJR', 'JJS']\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = []\n",
    "    for word in pos_tags:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0], pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))  # default POS = noun\n",
    "    return lemmatized_text\n",
    "\n",
    "def stop_word_removal(pos_tags_lem, lem_text):\n",
    "    stopwords = []\n",
    "\n",
    "    wanted_POS = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'VBG', 'FW']\n",
    "\n",
    "    for word in pos_tags_lem:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "\n",
    "    punctuations = list(str(string.punctuation))\n",
    "\n",
    "    stopwords = stopwords + punctuations\n",
    "\n",
    "    stopword_file = open(\"./dataset/long_stopwords.txt\", \"r\")\n",
    "    # Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "    lots_of_stopwords = []\n",
    "\n",
    "    for line in stopword_file.readlines():\n",
    "        lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "    stopwords_plus = []\n",
    "    stopwords_plus = stopwords + lots_of_stopwords\n",
    "    stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "    processed_text = []\n",
    "    for word in lem_text:\n",
    "        if word not in stopwords_plus:\n",
    "            processed_text.append(word)\n",
    "\n",
    "    return processed_text, stopwords_plus\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load = open(\"dataset/binary_class_dataset.pickle\", \"rb\")\n",
    "    tasks = pickle.load(load)\n",
    "    load.close()\n",
    "    print(tasks[0])\n",
    "    print(\"tasks loaded.....\")\n",
    "\n",
    "    preprocessed_tasks = []\n",
    "\n",
    "    all_words = []\n",
    "    freq_words_without_freq = []\n",
    "\n",
    "    for index in range(len(tasks)):\n",
    "\n",
    "        # without preprocessing\n",
    "        old_task = tasks[index]\n",
    "        text = tasks[index][0].lower()\n",
    "        cleaned_text = clean_text(text)\n",
    "        words = re.findall(r'\\w+', cleaned_text)\n",
    "\n",
    "               # without preprocessing\n",
    "        for item in words:\n",
    "\n",
    "            all_words.append(item)\n",
    "\n",
    "\n",
    "    counts = Counter(all_words).most_common(5000)\n",
    "\n",
    "    for index in range(len(counts)):\n",
    "        freq_words_without_freq.append(counts[index][0])\n",
    "\n",
    "\n",
    "    file = open(\"./dataset/freq_words_without_preprocessing.pickle\", \"wb\")\n",
    "    pickle.dump(freq_words_without_freq, file)\n",
    "    file.close()\n",
    "    print(\"all_phrases_without_freq saved......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0c93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
